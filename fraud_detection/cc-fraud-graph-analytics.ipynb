{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T00:42:35.925578Z","iopub.execute_input":"2022-02-24T00:42:35.926373Z","iopub.status.idle":"2022-02-24T00:42:36.212778Z","shell.execute_reply.started":"2022-02-24T00:42:35.926311Z","shell.execute_reply":"2022-02-24T00:42:36.212040Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv', delimiter=',')\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')\nprint('Event Rate:', np.mean(df.is_fraud))\ndisplay(df.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:42:38.416187Z","iopub.execute_input":"2022-02-24T00:42:38.416450Z","iopub.status.idle":"2022-02-24T00:42:49.068114Z","shell.execute_reply.started":"2022-02-24T00:42:38.416422Z","shell.execute_reply":"2022-02-24T00:42:49.067190Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**The event rate is 0.5%, so the dataset is highly imbalanced, we will random pick 20% non-event observations and all event observations to do undersampling.**","metadata":{}},{"cell_type":"code","source":"train_data = df[df.is_fraud==0].sample(frac=0.2, random_state = 2).append(df[df.is_fraud==1])\nprint('Event Rate:', np.mean(train_data.is_fraud))\nprint('Event Rate Distribution:\\n', train_data.is_fraud.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:42:51.733765Z","iopub.execute_input":"2022-02-24T00:42:51.734610Z","iopub.status.idle":"2022-02-24T00:42:54.168424Z","shell.execute_reply.started":"2022-02-24T00:42:51.734554Z","shell.execute_reply":"2022-02-24T00:42:54.162943Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Prepare Graph Data","metadata":{}},{"cell_type":"code","source":"def build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df = df_input.copy()\n    mapping = {x: node_id for node_id, x in enumerate(set(df['cc_num'].values.tolist()+\n                                                          df['merchant'].values.tolist()))}\n    df['from'] = df['cc_num'].apply(lambda x: mapping[x])\n    df['to'] = df['merchant'].apply(lambda x: mapping[x])\n    df = df[['from','to','amt','is_fraud']].groupby(['from','to']).agg({'is_fraud':'sum','amt':'sum'}).reset_index()\n    df['is_fraud'] = df['is_fraud'].apply(lambda x: 1 if x>0 else 0)\n    \n    G = nx.from_edgelist(df[['from','to']].values, create_using = graph_type)\n    \n    nx.set_node_attributes(G, {x:1 for x in df['from'].unique()}, 'bipartite')\n    nx.set_node_attributes(G, {x:2 for x in df['to'].unique()}, 'bipartite')\n    \n    nx.set_edge_attributes(G, \n                          {(int(x['from']), int(x['to'])): x['is_fraud'] for idx, x in df[['from','to','is_fraud']].iterrows()},\n                          'label')\n    nx.set_edge_attributes(G, \n                          {(int(x['from']), int(x['to'])): x['amt'] for idx, x in df[['from','to','amt']].iterrows()},\n                          'weight')\n    return(G)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:42:57.792671Z","iopub.execute_input":"2022-02-24T00:42:57.793197Z","iopub.status.idle":"2022-02-24T00:42:57.809055Z","shell.execute_reply.started":"2022-02-24T00:42:57.793165Z","shell.execute_reply":"2022-02-24T00:42:57.807906Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df = df_input.copy()\n    mapping = {x: node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                          df['cc_num'].values.tolist()+\n                                                          df['merchant'].values.tolist()))}\n    \n    df['in_node'] = df['cc_num'].apply(lambda x: mapping[x])\n    df['out_node'] = df['merchant'].apply(lambda x: mapping[x])\n       \n    G = nx.from_edgelist([(x['in_node'], mapping[idx]) for idx, x in df.iterrows()] +\n                         [(x['out_node'], mapping[idx]) for idx, x in df.iterrows()],\n                         create_using = graph_type)\n    \n    nx.set_node_attributes(G, {x['in_node']:1 for idx, x in df.iterrows()}, 'bipartite')\n    nx.set_node_attributes(G, {x['out_node']:2 for idx, x in df.iterrows()}, 'bipartite')\n    nx.set_node_attributes(G, {mapping[idx]:3 for idx, x in df.iterrows()}, 'bipartite')\n    \n    nx.set_edge_attributes(G, \n                          {(int(x['in_node']), mapping[idx]): x['is_fraud'] for idx, x in df.iterrows()},\n                          'label')\n    nx.set_edge_attributes(G, \n                          {(int(x['out_node']), mapping[idx]): x['is_fraud'] for idx, x in df.iterrows()},\n                          'label')\n   \n    nx.set_edge_attributes(G, \n                          {(int(x['in_node']), mapping[idx]): x['amt'] for idx, x in df.iterrows()},\n                          'weight')\n    nx.set_edge_attributes(G, \n                          {(int(x['out_node']), mapping[idx]): x['amt'] for idx, x in df.iterrows()},\n                          'weight')\n    \n    return(G)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:43:00.033531Z","iopub.execute_input":"2022-02-24T00:43:00.034320Z","iopub.status.idle":"2022-02-24T00:43:00.051754Z","shell.execute_reply.started":"2022-02-24T00:43:00.034270Z","shell.execute_reply":"2022-02-24T00:43:00.050812Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"G_bu = build_graph_bipartite(train_data, nx.Graph(name=['Bipartite Undirected']))\nG_bd = build_graph_bipartite(train_data, nx.DiGraph(name=['Bipartite Directed']))\nG_tu = build_graph_tripartite(train_data, nx.Graph(name=['Tripartite Undirected']))\nG_td = build_graph_tripartite(train_data, nx.DiGraph(name=['Tripartite Directed']))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:53:35.505864Z","iopub.execute_input":"2022-02-23T22:53:35.506410Z","iopub.status.idle":"2022-02-23T22:58:10.743918Z","shell.execute_reply.started":"2022-02-23T22:53:35.506356Z","shell.execute_reply":"2022-02-23T22:58:10.742890Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from networkx.algorithms import bipartite\nbipartite.is_bipartite(G_bu)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:58:27.852072Z","iopub.execute_input":"2022-02-23T22:58:27.852390Z","iopub.status.idle":"2022-02-23T22:58:27.989504Z","shell.execute_reply.started":"2022-02-23T22:58:27.852356Z","shell.execute_reply":"2022-02-23T22:58:27.988580Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(nx.info(G_bu))\nprint('==========================')\nprint(nx.info(G_tu))","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:58:29.767191Z","iopub.execute_input":"2022-02-23T22:58:29.767515Z","iopub.status.idle":"2022-02-23T22:58:30.007618Z","shell.execute_reply.started":"2022-02-23T22:58:29.767474Z","shell.execute_reply":"2022-02-23T22:58:30.006719Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Node Degree Distribution**\n\nFrom the below plot we can see, the bipartite graph has a more variegated distribution, with a peak of around 300. While tripartite graph has a peak of degree 2.","metadata":{}},{"cell_type":"code","source":"for G in [G_bu, G_tu]:\n    plt.figure(figsize = (10,10))\n    degrees = pd.Series(\n        {\n            k:v for k,v in nx.degree(G)\n        }\n    )\n    degrees.plot.hist()\n    plt.yscale(\"log\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:58:31.734710Z","iopub.execute_input":"2022-02-23T22:58:31.735048Z","iopub.status.idle":"2022-02-23T22:58:33.369544Z","shell.execute_reply.started":"2022-02-23T22:58:31.735010Z","shell.execute_reply":"2022-02-23T22:58:33.368410Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Edge Weight Distribution**\n\nFrom the below plot, the distribution is slightly shifted to the right (right-skewed) when compared to the tripartite where the transaction nodes are more pronounced.","metadata":{}},{"cell_type":"code","source":"for G in [G_bu, G_tu]:\n    allEdgesWeights = pd.Series({\n        (d[0], d[1]): d[2]['weight'] for d in G.edges(data=True)\n    })\n    np.quantile(allEdgesWeights.values, [0.1, 0.5, 0.7, 0.9, 1])\n    quant_dist = np.quantile(allEdgesWeights.values, [0.1, 0.5, 0.7, 0.9])\n    allEdgesWeightsFiltered = pd.Series({\n        (d[0], d[1]): d[2]['weight'] for d in G.edges(data=True) if d[2]['weight'] < quant_dist[-1]\n    })\n    plt.figure(figsize = (10,10))\n    allEdgesWeightsFiltered.plot.hist(bins=40)\n    plt.yscale('log')","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:58:36.023336Z","iopub.execute_input":"2022-02-23T22:58:36.023628Z","iopub.status.idle":"2022-02-23T22:58:48.851720Z","shell.execute_reply.started":"2022-02-23T22:58:36.023597Z","shell.execute_reply":"2022-02-23T22:58:48.850534Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Degree Centrality**\n\nDegree Centrality is defined as the number of links incident upon a node (i.e., the number of ties that a node has). \n","metadata":{}},{"cell_type":"code","source":"for G in [G_bu, G_tu]:\n    plt.figure(figsize = (10,10))\n    degree_centrality = pd.Series(\n        {\n            k:v for k,v in nx.degree_centrality(G).items()\n        }\n    )\n    degree_centrality.plot.hist()\n    plt.yscale(\"log\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:58:59.219996Z","iopub.execute_input":"2022-02-23T22:58:59.220863Z","iopub.status.idle":"2022-02-23T22:59:01.001489Z","shell.execute_reply.started":"2022-02-23T22:58:59.220789Z","shell.execute_reply":"2022-02-23T22:59:01.000497Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Betweenness Centrality**\n\nIt measures the number of shortest paths that pass through a given node, providing and intuition about how central that node is for message passing within the network.","metadata":{}},{"cell_type":"code","source":"# it takes forever to run this code\n# for G in [G_bu, G_tu]:\n#     plt.figure(figsize = (10,10))\n#     degree_centrality = pd.Series(\n#         {\n#             k:v for k,v in nx.betweenness_centrality(G).items()\n#         }\n#     )\n#     degree_centrality.plot.hist()\n#     plt.yscale(\"log\")","metadata":{"execution":{"iopub.status.busy":"2022-02-22T23:42:54.177297Z","iopub.execute_input":"2022-02-22T23:42:54.17754Z","iopub.status.idle":"2022-02-23T00:33:09.373363Z","shell.execute_reply.started":"2022-02-22T23:42:54.177512Z","shell.execute_reply":"2022-02-23T00:33:09.37223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Closeness Centrality**\n\nIt is a way of detecting nodes that are able to spread information very efficiently through a graph. the closeness centrality of a node measures its average farness (inverse distance) to all other nodes. Nodes with a high closeness score have the shortest distances to all other nodes. ","metadata":{}},{"cell_type":"code","source":"# it takes forever to run this code\n# for G in [G_bu, G_tu]:\n#     plt.figure(figsize = (10,10))\n#     degree_centrality = pd.Series(\n#         {\n#             k:v for k,v in nx.closeness_centrality(G).items()\n#         }\n#     )\n#     degree_centrality.plot.hist()\n#     plt.yscale(\"log\")","metadata":{"execution":{"iopub.status.busy":"2022-02-23T22:59:03.996880Z","iopub.execute_input":"2022-02-23T22:59:03.997188Z","iopub.status.idle":"2022-02-23T23:00:35.922528Z","shell.execute_reply.started":"2022-02-23T22:59:03.997155Z","shell.execute_reply":"2022-02-23T23:00:35.921276Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Assortativity**: \n\nis a preference for a network's nodes to attach to others that are similar in some way. Though the specific measure of similarity may vary, network theorists often examin assotativity interms of a node's degree.","metadata":{}},{"cell_type":"code","source":"nx.degree_pearson_correlation_coefficient(G_bu)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T23:00:40.587211Z","iopub.execute_input":"2022-02-23T23:00:40.587657Z","iopub.status.idle":"2022-02-23T23:00:43.070026Z","shell.execute_reply.started":"2022-02-23T23:00:40.587623Z","shell.execute_reply":"2022-02-23T23:00:43.069146Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"nx.degree_pearson_correlation_coefficient(G_tu)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T23:00:55.931929Z","iopub.execute_input":"2022-02-23T23:00:55.932219Z","iopub.status.idle":"2022-02-23T23:01:03.842979Z","shell.execute_reply.started":"2022-02-23T23:00:55.932190Z","shell.execute_reply":"2022-02-23T23:01:03.841909Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Community Detection & Fraudulent Edges Ratio Analysis\n\nCommunities are extracted based on the edge weights using community.best_partition command","metadata":{}},{"cell_type":"code","source":"# pip install python-louvain\nimport community\nparts = community.best_partition(G_bu, random_state = 4, weight = 'weight')\ncommunities = pd.Series(parts)\ncommunities.value_counts().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T23:01:54.132343Z","iopub.execute_input":"2022-02-23T23:01:54.132628Z","iopub.status.idle":"2022-02-23T23:02:10.867450Z","shell.execute_reply.started":"2022-02-23T23:01:54.132598Z","shell.execute_reply":"2022-02-23T23:02:10.866628Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"From the above result, there are 19 communities in total, communities 12 has the highest weight where community 15 has the lowest","metadata":{}},{"cell_type":"code","source":"communities.value_counts().plot.hist(bins=20)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T23:03:12.588891Z","iopub.execute_input":"2022-02-23T23:03:12.589187Z","iopub.status.idle":"2022-02-23T23:03:12.848269Z","shell.execute_reply.started":"2022-02-23T23:03:12.589157Z","shell.execute_reply":"2022-02-23T23:03:12.847374Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The following code generates a node-inducedsubgraph by using the nodes present in a particular community. Since in bipartite graph, all the transactions are donoted by the corresponding edges between the nodes. we will take the fraud_edges / tmp.number_of_edges() for computing the fraudulent ratio percentage.\n\nThe output of the below code,community 0 has the most fraudulent transaction percentage with 21.6%, closely followed by community 16 and community 14. Community 15 has the least percentage of fraudulent transactions.","metadata":{}},{"cell_type":"code","source":"graphs = []\nd = {}\nfor x in communities.unique():\n    tmp = nx.subgraph(G_bu, communities[communities == x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, 'label').values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n    \npd.Series(d).sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T23:05:47.784811Z","iopub.execute_input":"2022-02-23T23:05:47.785111Z","iopub.status.idle":"2022-02-23T23:05:48.477895Z","shell.execute_reply.started":"2022-02-23T23:05:47.785078Z","shell.execute_reply":"2022-02-23T23:05:48.477026Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"gID = 1\nplt.figure(figsize = (10,10))\nspring_pos = nx.spring_layout(graphs[gID])\nplt.axis('off')\nedge_colors = ['r' if x==1 else 'g' for x in nx.get_edge_attributes(graphs[gID], 'label').values()]\nnx.draw_networkx(graphs[gID], pos=spring_pos, \n                edge_color=edge_colors, with_labels = True, node_size = 15)","metadata":{"execution":{"iopub.status.busy":"2022-02-23T23:13:17.746845Z","iopub.execute_input":"2022-02-23T23:13:17.747126Z","iopub.status.idle":"2022-02-23T23:13:18.440894Z","shell.execute_reply.started":"2022-02-23T23:13:17.747095Z","shell.execute_reply":"2022-02-23T23:13:18.440091Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"* Community Detection is similar to the Clustering method in unsupervised learning\n* Different communities are identified with various levels of density\n* All the communities are evaluated for the Ratio of Fraud to Benign transaction edges and shown in plots\n## Model Development -- Supervised Learning\n### Prepare Dataset (Undersampling and split training/testing)","metadata":{}},{"cell_type":"code","source":"from sklearn.utils import resample\ndf_majority = df[df.is_fraud == 0]\ndf_minority = df[df.is_fraud == 1]\ndf_maj_dowsampled = resample(df_majority,\n                            n_samples = len(df_minority),\n                            random_state = 42)\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:43:21.085533Z","iopub.execute_input":"2022-02-24T00:43:21.085890Z","iopub.status.idle":"2022-02-24T00:43:23.085486Z","shell.execute_reply.started":"2022-02-24T00:43:21.085852Z","shell.execute_reply":"2022-02-24T00:43:23.084545Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))),\n                                                                      list(nx.get_edge_attributes(G_down, 'label').values()),\n                                                                      test_size = 0.2,\n                                                                      random_state = 4\n                                                                     )","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:45:03.031006Z","iopub.execute_input":"2022-02-24T00:45:03.031315Z","iopub.status.idle":"2022-02-24T00:45:03.078542Z","shell.execute_reply.started":"2022-02-24T00:45:03.031286Z","shell.execute_reply":"2022-02-24T00:45:03.077869Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Node to Vector Embedding**","metadata":{}},{"cell_type":"code","source":"!pip install node2vec\n!pip install python-Levenshtein\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\n\nnode2vec_train = Node2Vec(G_down, weight_key = 'weight')\nmodel_train = node2vec_train.fit(window = 10)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T00:57:13.770712Z","iopub.execute_input":"2022-02-24T00:57:13.771063Z","iopub.status.idle":"2022-02-24T00:59:34.436578Z","shell.execute_reply.started":"2022-02-24T00:57:13.771027Z","shell.execute_reply":"2022-02-24T00:59:34.435425Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Supervised Learning","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nedgs =list(G_down.edges)\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors = model_train.wv)\n    \n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators = 1000, random_state = 4)\n    rf.fit(train_embeddings, train_labels)\n    \n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred))\n    print('Recall:', metrics.recall_score(test_labels, y_pred))\n    print('F1-Score:', metrics.f1_score(test_labels, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-02-24T02:02:03.646211Z","iopub.execute_input":"2022-02-24T02:02:03.646561Z","iopub.status.idle":"2022-02-24T02:12:55.731868Z","shell.execute_reply.started":"2022-02-24T02:02:03.646527Z","shell.execute_reply":"2022-02-24T02:12:55.730912Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"### Unsupervised Learning","metadata":{}},{"cell_type":"code","source":"node2vec_unsup = Node2Vec(G_down, weight_key = 'weight')\nunsup_vals = node2vec_unsup.fit(window = 10)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T02:13:24.927779Z","iopub.execute_input":"2022-02-24T02:13:24.928369Z","iopub.status.idle":"2022-02-24T02:15:31.436612Z","shell.execute_reply.started":"2022-02-24T02:13:24.928312Z","shell.execute_reply":"2022-02-24T02:15:31.435876Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, 'label').values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors = unsup_vals.wv)\n    \n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state = 4).fit(embedding)\n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T02:18:06.491546Z","iopub.execute_input":"2022-02-24T02:18:06.491930Z","iopub.status.idle":"2022-02-24T02:18:15.679628Z","shell.execute_reply.started":"2022-02-24T02:18:06.491890Z","shell.execute_reply":"2022-02-24T02:18:15.679015Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"for unsupervised KMeans approach, WeightedL1Embedder has the best performance.","metadata":{}}]}