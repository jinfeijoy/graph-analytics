{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-24T00:42:35.926373Z",
     "iopub.status.busy": "2022-02-24T00:42:35.925578Z",
     "iopub.status.idle": "2022-02-24T00:42:36.212778Z",
     "shell.execute_reply": "2022-02-24T00:42:36.212040Z",
     "shell.execute_reply.started": "2022-02-24T00:42:35.926311Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:42:38.416450Z",
     "iopub.status.busy": "2022-02-24T00:42:38.416187Z",
     "iopub.status.idle": "2022-02-24T00:42:49.068114Z",
     "shell.execute_reply": "2022-02-24T00:42:49.067190Z",
     "shell.execute_reply.started": "2022-02-24T00:42:38.416422Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/fraud-detection/fraudTrain.csv', delimiter=',')\n",
    "nRow, nCol = df.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "print('Event Rate:', np.mean(df.is_fraud))\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The event rate is 0.5%, so the dataset is highly imbalanced, we will random pick 20% non-event observations and all event observations to do undersampling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:42:51.734610Z",
     "iopub.status.busy": "2022-02-24T00:42:51.733765Z",
     "iopub.status.idle": "2022-02-24T00:42:54.168424Z",
     "shell.execute_reply": "2022-02-24T00:42:54.162943Z",
     "shell.execute_reply.started": "2022-02-24T00:42:51.734554Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = df[df.is_fraud==0].sample(frac=0.2, random_state = 2).append(df[df.is_fraud==1])\n",
    "print('Event Rate:', np.mean(train_data.is_fraud))\n",
    "print('Event Rate Distribution:\\n', train_data.is_fraud.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Graph Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:42:57.793197Z",
     "iopub.status.busy": "2022-02-24T00:42:57.792671Z",
     "iopub.status.idle": "2022-02-24T00:42:57.809055Z",
     "shell.execute_reply": "2022-02-24T00:42:57.807906Z",
     "shell.execute_reply.started": "2022-02-24T00:42:57.793165Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_graph_bipartite(df_input, graph_type=nx.Graph()):\n",
    "    df = df_input.copy()\n",
    "    mapping = {x: node_id for node_id, x in enumerate(set(df['cc_num'].values.tolist()+\n",
    "                                                          df['merchant'].values.tolist()))}\n",
    "    df['from'] = df['cc_num'].apply(lambda x: mapping[x])\n",
    "    df['to'] = df['merchant'].apply(lambda x: mapping[x])\n",
    "    df = df[['from','to','amt','is_fraud']].groupby(['from','to']).agg({'is_fraud':'sum','amt':'sum'}).reset_index()\n",
    "    df['is_fraud'] = df['is_fraud'].apply(lambda x: 1 if x>0 else 0)\n",
    "    \n",
    "    G = nx.from_edgelist(df[['from','to']].values, create_using = graph_type)\n",
    "    \n",
    "    nx.set_node_attributes(G, {x:1 for x in df['from'].unique()}, 'bipartite')\n",
    "    nx.set_node_attributes(G, {x:2 for x in df['to'].unique()}, 'bipartite')\n",
    "    \n",
    "    nx.set_edge_attributes(G, \n",
    "                          {(int(x['from']), int(x['to'])): x['is_fraud'] for idx, x in df[['from','to','is_fraud']].iterrows()},\n",
    "                          'label')\n",
    "    nx.set_edge_attributes(G, \n",
    "                          {(int(x['from']), int(x['to'])): x['amt'] for idx, x in df[['from','to','amt']].iterrows()},\n",
    "                          'weight')\n",
    "    return(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:43:00.034320Z",
     "iopub.status.busy": "2022-02-24T00:43:00.033531Z",
     "iopub.status.idle": "2022-02-24T00:43:00.051754Z",
     "shell.execute_reply": "2022-02-24T00:43:00.050812Z",
     "shell.execute_reply.started": "2022-02-24T00:43:00.034270Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_graph_tripartite(df_input, graph_type=nx.Graph()):\n",
    "    df = df_input.copy()\n",
    "    mapping = {x: node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n",
    "                                                          df['cc_num'].values.tolist()+\n",
    "                                                          df['merchant'].values.tolist()))}\n",
    "    \n",
    "    df['in_node'] = df['cc_num'].apply(lambda x: mapping[x])\n",
    "    df['out_node'] = df['merchant'].apply(lambda x: mapping[x])\n",
    "       \n",
    "    G = nx.from_edgelist([(x['in_node'], mapping[idx]) for idx, x in df.iterrows()] +\n",
    "                         [(x['out_node'], mapping[idx]) for idx, x in df.iterrows()],\n",
    "                         create_using = graph_type)\n",
    "    \n",
    "    nx.set_node_attributes(G, {x['in_node']:1 for idx, x in df.iterrows()}, 'bipartite')\n",
    "    nx.set_node_attributes(G, {x['out_node']:2 for idx, x in df.iterrows()}, 'bipartite')\n",
    "    nx.set_node_attributes(G, {mapping[idx]:3 for idx, x in df.iterrows()}, 'bipartite')\n",
    "    \n",
    "    nx.set_edge_attributes(G, \n",
    "                          {(int(x['in_node']), mapping[idx]): x['is_fraud'] for idx, x in df.iterrows()},\n",
    "                          'label')\n",
    "    nx.set_edge_attributes(G, \n",
    "                          {(int(x['out_node']), mapping[idx]): x['is_fraud'] for idx, x in df.iterrows()},\n",
    "                          'label')\n",
    "   \n",
    "    nx.set_edge_attributes(G, \n",
    "                          {(int(x['in_node']), mapping[idx]): x['amt'] for idx, x in df.iterrows()},\n",
    "                          'weight')\n",
    "    nx.set_edge_attributes(G, \n",
    "                          {(int(x['out_node']), mapping[idx]): x['amt'] for idx, x in df.iterrows()},\n",
    "                          'weight')\n",
    "    \n",
    "    return(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:53:35.506410Z",
     "iopub.status.busy": "2022-02-23T22:53:35.505864Z",
     "iopub.status.idle": "2022-02-23T22:58:10.743918Z",
     "shell.execute_reply": "2022-02-23T22:58:10.742890Z",
     "shell.execute_reply.started": "2022-02-23T22:53:35.506356Z"
    }
   },
   "outputs": [],
   "source": [
    "G_bu = build_graph_bipartite(train_data, nx.Graph(name=['Bipartite Undirected']))\n",
    "G_bd = build_graph_bipartite(train_data, nx.DiGraph(name=['Bipartite Directed']))\n",
    "G_tu = build_graph_tripartite(train_data, nx.Graph(name=['Tripartite Undirected']))\n",
    "G_td = build_graph_tripartite(train_data, nx.DiGraph(name=['Tripartite Directed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:58:27.852390Z",
     "iopub.status.busy": "2022-02-23T22:58:27.852072Z",
     "iopub.status.idle": "2022-02-23T22:58:27.989504Z",
     "shell.execute_reply": "2022-02-23T22:58:27.988580Z",
     "shell.execute_reply.started": "2022-02-23T22:58:27.852356Z"
    }
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "bipartite.is_bipartite(G_bu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:58:29.767515Z",
     "iopub.status.busy": "2022-02-23T22:58:29.767191Z",
     "iopub.status.idle": "2022-02-23T22:58:30.007618Z",
     "shell.execute_reply": "2022-02-23T22:58:30.006719Z",
     "shell.execute_reply.started": "2022-02-23T22:58:29.767474Z"
    }
   },
   "outputs": [],
   "source": [
    "print(nx.info(G_bu))\n",
    "print('==========================')\n",
    "print(nx.info(G_tu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node Degree Distribution**\n",
    "\n",
    "From the below plot we can see, the bipartite graph has a more variegated distribution, with a peak of around 300. While tripartite graph has a peak of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:58:31.735048Z",
     "iopub.status.busy": "2022-02-23T22:58:31.734710Z",
     "iopub.status.idle": "2022-02-23T22:58:33.369544Z",
     "shell.execute_reply": "2022-02-23T22:58:33.368410Z",
     "shell.execute_reply.started": "2022-02-23T22:58:31.735010Z"
    }
   },
   "outputs": [],
   "source": [
    "for G in [G_bu, G_tu]:\n",
    "    plt.figure(figsize = (10,10))\n",
    "    degrees = pd.Series(\n",
    "        {\n",
    "            k:v for k,v in nx.degree(G)\n",
    "        }\n",
    "    )\n",
    "    degrees.plot.hist()\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge Weight Distribution**\n",
    "\n",
    "From the below plot, the distribution is slightly shifted to the right (right-skewed) when compared to the tripartite where the transaction nodes are more pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:58:36.023628Z",
     "iopub.status.busy": "2022-02-23T22:58:36.023336Z",
     "iopub.status.idle": "2022-02-23T22:58:48.851720Z",
     "shell.execute_reply": "2022-02-23T22:58:48.850534Z",
     "shell.execute_reply.started": "2022-02-23T22:58:36.023597Z"
    }
   },
   "outputs": [],
   "source": [
    "for G in [G_bu, G_tu]:\n",
    "    allEdgesWeights = pd.Series({\n",
    "        (d[0], d[1]): d[2]['weight'] for d in G.edges(data=True)\n",
    "    })\n",
    "    np.quantile(allEdgesWeights.values, [0.1, 0.5, 0.7, 0.9, 1])\n",
    "    quant_dist = np.quantile(allEdgesWeights.values, [0.1, 0.5, 0.7, 0.9])\n",
    "    allEdgesWeightsFiltered = pd.Series({\n",
    "        (d[0], d[1]): d[2]['weight'] for d in G.edges(data=True) if d[2]['weight'] < quant_dist[-1]\n",
    "    })\n",
    "    plt.figure(figsize = (10,10))\n",
    "    allEdgesWeightsFiltered.plot.hist(bins=40)\n",
    "    plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Degree Centrality**\n",
    "\n",
    "Degree Centrality is defined as the number of links incident upon a node (i.e., the number of ties that a node has). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:58:59.220863Z",
     "iopub.status.busy": "2022-02-23T22:58:59.219996Z",
     "iopub.status.idle": "2022-02-23T22:59:01.001489Z",
     "shell.execute_reply": "2022-02-23T22:59:01.000497Z",
     "shell.execute_reply.started": "2022-02-23T22:58:59.220789Z"
    }
   },
   "outputs": [],
   "source": [
    "for G in [G_bu, G_tu]:\n",
    "    plt.figure(figsize = (10,10))\n",
    "    degree_centrality = pd.Series(\n",
    "        {\n",
    "            k:v for k,v in nx.degree_centrality(G).items()\n",
    "        }\n",
    "    )\n",
    "    degree_centrality.plot.hist()\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Betweenness Centrality**\n",
    "\n",
    "It measures the number of shortest paths that pass through a given node, providing and intuition about how central that node is for message passing within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-22T23:42:54.17754Z",
     "iopub.status.busy": "2022-02-22T23:42:54.177297Z",
     "iopub.status.idle": "2022-02-23T00:33:09.373363Z",
     "shell.execute_reply": "2022-02-23T00:33:09.37223Z",
     "shell.execute_reply.started": "2022-02-22T23:42:54.177512Z"
    }
   },
   "outputs": [],
   "source": [
    "# it takes forever to run this code\n",
    "# for G in [G_bu, G_tu]:\n",
    "#     plt.figure(figsize = (10,10))\n",
    "#     degree_centrality = pd.Series(\n",
    "#         {\n",
    "#             k:v for k,v in nx.betweenness_centrality(G).items()\n",
    "#         }\n",
    "#     )\n",
    "#     degree_centrality.plot.hist()\n",
    "#     plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Closeness Centrality**\n",
    "\n",
    "It is a way of detecting nodes that are able to spread information very efficiently through a graph. the closeness centrality of a node measures its average farness (inverse distance) to all other nodes. Nodes with a high closeness score have the shortest distances to all other nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T22:59:03.997188Z",
     "iopub.status.busy": "2022-02-23T22:59:03.996880Z",
     "iopub.status.idle": "2022-02-23T23:00:35.922528Z",
     "shell.execute_reply": "2022-02-23T23:00:35.921276Z",
     "shell.execute_reply.started": "2022-02-23T22:59:03.997155Z"
    }
   },
   "outputs": [],
   "source": [
    "# it takes forever to run this code\n",
    "# for G in [G_bu, G_tu]:\n",
    "#     plt.figure(figsize = (10,10))\n",
    "#     degree_centrality = pd.Series(\n",
    "#         {\n",
    "#             k:v for k,v in nx.closeness_centrality(G).items()\n",
    "#         }\n",
    "#     )\n",
    "#     degree_centrality.plot.hist()\n",
    "#     plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assortativity**: \n",
    "\n",
    "is a preference for a network's nodes to attach to others that are similar in some way. Though the specific measure of similarity may vary, network theorists often examin assotativity interms of a node's degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T23:00:40.587657Z",
     "iopub.status.busy": "2022-02-23T23:00:40.587211Z",
     "iopub.status.idle": "2022-02-23T23:00:43.070026Z",
     "shell.execute_reply": "2022-02-23T23:00:43.069146Z",
     "shell.execute_reply.started": "2022-02-23T23:00:40.587623Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.degree_pearson_correlation_coefficient(G_bu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T23:00:55.932219Z",
     "iopub.status.busy": "2022-02-23T23:00:55.931929Z",
     "iopub.status.idle": "2022-02-23T23:01:03.842979Z",
     "shell.execute_reply": "2022-02-23T23:01:03.841909Z",
     "shell.execute_reply.started": "2022-02-23T23:00:55.932190Z"
    }
   },
   "outputs": [],
   "source": [
    "nx.degree_pearson_correlation_coefficient(G_tu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection & Fraudulent Edges Ratio Analysis\n",
    "\n",
    "Communities are extracted based on the edge weights using community.best_partition command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T23:01:54.132628Z",
     "iopub.status.busy": "2022-02-23T23:01:54.132343Z",
     "iopub.status.idle": "2022-02-23T23:02:10.867450Z",
     "shell.execute_reply": "2022-02-23T23:02:10.866628Z",
     "shell.execute_reply.started": "2022-02-23T23:01:54.132598Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install python-louvain\n",
    "import community\n",
    "parts = community.best_partition(G_bu, random_state = 4, weight = 'weight')\n",
    "communities = pd.Series(parts)\n",
    "communities.value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, there are 19 communities in total, communities 12 has the highest weight where community 15 has the lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T23:03:12.589187Z",
     "iopub.status.busy": "2022-02-23T23:03:12.588891Z",
     "iopub.status.idle": "2022-02-23T23:03:12.848269Z",
     "shell.execute_reply": "2022-02-23T23:03:12.847374Z",
     "shell.execute_reply.started": "2022-02-23T23:03:12.589157Z"
    }
   },
   "outputs": [],
   "source": [
    "communities.value_counts().plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code generates a node-inducedsubgraph by using the nodes present in a particular community. Since in bipartite graph, all the transactions are donoted by the corresponding edges between the nodes. we will take the fraud_edges / tmp.number_of_edges() for computing the fraudulent ratio percentage.\n",
    "\n",
    "The output of the below code,community 0 has the most fraudulent transaction percentage with 21.6%, closely followed by community 16 and community 14. Community 15 has the least percentage of fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T23:05:47.785111Z",
     "iopub.status.busy": "2022-02-23T23:05:47.784811Z",
     "iopub.status.idle": "2022-02-23T23:05:48.477895Z",
     "shell.execute_reply": "2022-02-23T23:05:48.477026Z",
     "shell.execute_reply.started": "2022-02-23T23:05:47.785078Z"
    }
   },
   "outputs": [],
   "source": [
    "graphs = []\n",
    "d = {}\n",
    "for x in communities.unique():\n",
    "    tmp = nx.subgraph(G_bu, communities[communities == x].index)\n",
    "    fraud_edges = sum(nx.get_edge_attributes(tmp, 'label').values())\n",
    "    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n",
    "    d[x] = ratio\n",
    "    graphs += [tmp]\n",
    "    \n",
    "pd.Series(d).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-23T23:13:17.747126Z",
     "iopub.status.busy": "2022-02-23T23:13:17.746845Z",
     "iopub.status.idle": "2022-02-23T23:13:18.440894Z",
     "shell.execute_reply": "2022-02-23T23:13:18.440091Z",
     "shell.execute_reply.started": "2022-02-23T23:13:17.747095Z"
    }
   },
   "outputs": [],
   "source": [
    "gID = 1\n",
    "plt.figure(figsize = (10,10))\n",
    "spring_pos = nx.spring_layout(graphs[gID])\n",
    "plt.axis('off')\n",
    "edge_colors = ['r' if x==1 else 'g' for x in nx.get_edge_attributes(graphs[gID], 'label').values()]\n",
    "nx.draw_networkx(graphs[gID], pos=spring_pos, \n",
    "                edge_color=edge_colors, with_labels = True, node_size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Community Detection is similar to the Clustering method in unsupervised learning\n",
    "* Different communities are identified with various levels of density\n",
    "* All the communities are evaluated for the Ratio of Fraud to Benign transaction edges and shown in plots\n",
    "## Model Development -- Supervised Learning\n",
    "### Prepare Dataset (Undersampling and split training/testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:43:21.085890Z",
     "iopub.status.busy": "2022-02-24T00:43:21.085533Z",
     "iopub.status.idle": "2022-02-24T00:43:23.085486Z",
     "shell.execute_reply": "2022-02-24T00:43:23.084545Z",
     "shell.execute_reply.started": "2022-02-24T00:43:21.085852Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "df_majority = df[df.is_fraud == 0]\n",
    "df_minority = df[df.is_fraud == 1]\n",
    "df_maj_dowsampled = resample(df_majority,\n",
    "                            n_samples = len(df_minority),\n",
    "                            random_state = 42)\n",
    "df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n",
    "print(df_downsampled.is_fraud.value_counts())\n",
    "G_down = build_graph_bipartite(df_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:45:03.031315Z",
     "iopub.status.busy": "2022-02-24T00:45:03.031006Z",
     "iopub.status.idle": "2022-02-24T00:45:03.078542Z",
     "shell.execute_reply": "2022-02-24T00:45:03.077869Z",
     "shell.execute_reply.started": "2022-02-24T00:45:03.031286Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))),\n",
    "                                                                      list(nx.get_edge_attributes(G_down, 'label').values()),\n",
    "                                                                      test_size = 0.2,\n",
    "                                                                      random_state = 4\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Node to Vector Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T00:57:13.771063Z",
     "iopub.status.busy": "2022-02-24T00:57:13.770712Z",
     "iopub.status.idle": "2022-02-24T00:59:34.436578Z",
     "shell.execute_reply": "2022-02-24T00:59:34.435425Z",
     "shell.execute_reply.started": "2022-02-24T00:57:13.771027Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install node2vec\n",
    "!pip install python-Levenshtein\n",
    "from node2vec import Node2Vec\n",
    "from node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n",
    "\n",
    "\n",
    "\n",
    "node2vec_train = Node2Vec(G_down, weight_key = 'weight')\n",
    "model_train = node2vec_train.fit(window = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T02:02:03.646561Z",
     "iopub.status.busy": "2022-02-24T02:02:03.646211Z",
     "iopub.status.idle": "2022-02-24T02:12:55.731868Z",
     "shell.execute_reply": "2022-02-24T02:12:55.730912Z",
     "shell.execute_reply.started": "2022-02-24T02:02:03.646527Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "edgs =list(G_down.edges)\n",
    "for cl in classes:\n",
    "    embeddings_train = cl(keyed_vectors = model_train.wv)\n",
    "    \n",
    "    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n",
    "    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators = 1000, random_state = 4)\n",
    "    rf.fit(train_embeddings, train_labels)\n",
    "    \n",
    "    y_pred = rf.predict(test_embeddings)\n",
    "    print(cl)\n",
    "    print('Precision:', metrics.precision_score(test_labels, y_pred))\n",
    "    print('Recall:', metrics.recall_score(test_labels, y_pred))\n",
    "    print('F1-Score:', metrics.f1_score(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T02:13:24.928369Z",
     "iopub.status.busy": "2022-02-24T02:13:24.927779Z",
     "iopub.status.idle": "2022-02-24T02:15:31.436612Z",
     "shell.execute_reply": "2022-02-24T02:15:31.435876Z",
     "shell.execute_reply.started": "2022-02-24T02:13:24.928312Z"
    }
   },
   "outputs": [],
   "source": [
    "node2vec_unsup = Node2Vec(G_down, weight_key = 'weight')\n",
    "unsup_vals = node2vec_unsup.fit(window = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-24T02:18:06.491930Z",
     "iopub.status.busy": "2022-02-24T02:18:06.491546Z",
     "iopub.status.idle": "2022-02-24T02:18:15.679628Z",
     "shell.execute_reply": "2022-02-24T02:18:15.679015Z",
     "shell.execute_reply.started": "2022-02-24T02:18:06.491890Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "classes = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\n",
    "true_labels = [x for x in nx.get_edge_attributes(G_down, 'label').values()]\n",
    "\n",
    "for cl in classes:\n",
    "    embedding_edge = cl(keyed_vectors = unsup_vals.wv)\n",
    "    \n",
    "    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n",
    "    kmeans = KMeans(2, random_state = 4).fit(embedding)\n",
    "    \n",
    "    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n",
    "    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n",
    "    co = metrics.completeness_score(true_labels, kmeans.labels_)\n",
    "    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n",
    "    \n",
    "    print(cl)\n",
    "    print('NMI:', nmi)\n",
    "    print('Homogeneity:', ho)\n",
    "    print('Completeness:', co)\n",
    "    print('V-Measure:', vmeasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for unsupervised KMeans approach, WeightedL1Embedder has the best performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
